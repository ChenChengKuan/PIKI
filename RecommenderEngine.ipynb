{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Engine\n",
    "We will put everything together to implement a recommender engine. Our Recommender engine is hybrid of follwoing 5 sub engines:<br>\n",
    "* Weighted User based recommender\n",
    "* Item based recommender\n",
    "* Desinger based recommender\n",
    "* Most Popular recommender\n",
    "* Expert recommender\n",
    "\n",
    "<br>\n",
    "First of all, we calculate the 25%,50%,75% percentile number of purchase, which is used to decide which combination of recommender engines are triggered. The decision tree below summarize the decision process(tid is the unique tag of product, browse is the browing record of user) and the technique used to surpress overspecialization\n",
    "<br>\n",
    "\n",
    "The left red arrows is user's familiarity of Pinkoi, the deeper it is , the more familiar hence the probability of overspecialization get higher. The right blues arrows are techniques to combat overspecilaization. The darker it is , the stonger its ability to supress overspecialization. Note that the parameter rand_n  vary in opposite direction. it neutralize the other two techniques to prevent the loss of preciseness\n",
    "<img src='Intermediate/RecDT.png' width=1860 height=980>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the branching criteria and the logic behind which recommender engine is triggered, we have to understand the core idea and covergae of each engine:\n",
    "<br>\n",
    "### Recommendation Engine\n",
    "\n",
    "#### MostPopularRec:\n",
    "Simply extract 6 most popular product pre-calculated in DataExploration. This is specially for first-come user without any records before\n",
    "\n",
    "#### ExpertRec\n",
    "This engine is for user who doesn't purchase any product before but has some browing records. Here we don't consider browing time(because I don't have data) so the subcategory that is interesting to user is simply depends on the number of it in browing records. For example, if the browing record is [a,a,b,c,d,h], then this user is interested in subcategory 'a'. We thus find the expert of this subcategory then recommend 6 products bought by the expert. The definition of expert here is the user who has the highest number purchase of this subcategory, which is pre-calculated in ExpertCalculation \n",
    "\n",
    "#### Userbased recommender:\n",
    "The goal of this engie is to provide precise recommendation.It is based on the nearsest usres pre-calculated in UserSimilarityCalculation. It searches the prurchase records of top 2 nearest users and calculate find the k most similar products among these products.\n",
    "\n",
    "#### Itembased recommender\n",
    "The goal of this engine is the same as Userbased recommender. Instead of searching similar user, it searches the product in subcategory user is most intereested in. Subcategory decided, the similarity only rely on the style of product and the style preference of user. Hence we calculate the cosine similarity of user's style preference and style of product(Product Tag here) then get the top K nearest products.\n",
    "\n",
    "#### Designer based recommender\n",
    "This engine aims to address the overspecialization problem caused by the previous two engines. It aims to provide novel and seredipitious recommendation list. The engine search the designer user likes most based on purchase record. It randomly choose the product from the union of this designer and the most similar designer. We choose randomly product instead of calculating cosine similarity as in item based recommender is because the goal of deisnger recommender is to increase the novelty and serendipity. After several experiments I found that if we calculate the cosine similarity again, the recommendataionlist would become too specialize. The reason why this engine can increase novelty and serendipity is based on weaker similarity metircs used here. It use designer, which is a broader class, instead of compare the product one on one, which is very specialize.\n",
    "\n",
    "Table below smmarize the property of each engine:\n",
    "<img src='Intermediate/recsum.png' width=600 height=400 >\n",
    "Now it's reasonable to understand the branching criteria in decisoin tree. As the tree go deeper, the scope of product discovered by user become broader. Consequently, we should reduce the number of product recommended by engine with stronger specialization(Userbased,Itembased here) and increase the lower one(Designer based) in case that the user has seen the recommended product before. **In conclusion, as the user buy more( be more familiar with our product), we should focus on more on overspecialization issue**\n",
    "\n",
    "\n",
    "\n",
    "Even though I apply the design aforementioned, over-specialization still happended. Hence I apply following two tricks to further surpress over-specialization:\n",
    "### Techniques to surpress overspecialization\n",
    "\n",
    "#### Hybrid of engine:\n",
    "This is simply implemented by deciding how many items an engine can return. If we let the engine with stronger specialization then the overall recommendation list get more specialized\n",
    "\n",
    "#### parameter of randomness: rand_p\n",
    "Instead of asking the recommendation engine to return k product, we ask the engine to return rand_p * k products and randomly choose k product within the set of rand_p * k. As rand_p get larger, it becomes more difficult to get the most similar products, which reduce overspecialization. This parameter get smaller as the degree of over specialization of overall combination of recommender engine get lower(i,e the last combination of engine has the smallest rand_p while the first has the largest one )\n",
    "\n",
    "#### Linearly decrese search scope of ProductRec:\n",
    "As the tree(stage) go deeper, the less product scope the recommender engine search. By doing so, it can reduce the probability of finding very similar data the user has seen before thus reduce overspecialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load all data pre-calculate data \n",
    "Productdata=pd.read_csv(\"Intermediate/data_final.csv\",sep=\",\",encoding='utf8',low_memory=False)\n",
    "Userdata=pd.read_csv(\"Intermediate/SimUserData.csv\",sep=\",\",encoding='utf8')\n",
    "Designer_sim=pd.read_csv(\"Intermediate/design_sim.csv\",sep=\",\",encoding='utf8')\n",
    "Designer_data=pd.read_csv(\"Intermediate/designer_data_list.csv\",sep=\",\",encoding='utf8',header=None)\n",
    "Popularity=pd.read_csv(\"Intermediate/Popularity.csv\",sep=\",\",encoding='utf8')\n",
    "UserSim=pd.read_csv(\"Intermediate/Similarity_user.csv\",sep=\",\",encoding='utf8')\n",
    "Expert=pd.read_csv(\"Intermediate/expert.csv\",sep=\",\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the index of Productdata by tid, which would be useful for later indexing\n",
    "Productdata.index=Productdata['tid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Productdata['Product Tag']=Productdata['Product Tag'].fillna(u'0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function calculate the percentile of numeber of purchase\n",
    "def PurchaseStat(X):\n",
    "    l=[len(e.split(\",\")) for e in X]\n",
    "    return [np.percentile(l,25),np.percentile(l,50),np.percentile(l,75)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_stat=PurchaseStat(Userdata['tid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0, 8.0, 12.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Following are three helper functions, which aims to achieve the fast sparse array calculation\n",
    "#dic_converter: convert list of string to dictionary. the fromkeys, for.. method has been proved to be the fatest method \n",
    "#dic_sim:calculate the cosine similarity of two dic objects\n",
    "#dic_max: return the key with maximum values, randomly choose one if more than one key has maximum values\n",
    "def dic_converter(x):\n",
    "    if x is np.nan:\n",
    "        x=u'0'\n",
    "    L=x.split(\",\")\n",
    "    D={}.fromkeys(L,0)\n",
    "    \n",
    "    for e in L:\n",
    "        D[e]+=1.0\n",
    "    return D \n",
    "\n",
    "def dic_sim(x,y):\n",
    "    common=set(x.keys()).intersection(set(y.keys()))\n",
    "    norm_x=np.sqrt(sum([e*e for e in x.values()]))\n",
    "    norm_y=np.sqrt(sum([e*e for e in y.values()]))\n",
    "    \n",
    "    s=0.0\n",
    "    for c in common:\n",
    "        s+=x[c]*y[c]\n",
    "    s/=(norm_x*norm_y)\n",
    "\n",
    "    return s\n",
    "\n",
    "def dic_max(x):\n",
    "    maximum=max(x.values())\n",
    "    d_max=[k for k,v in x.items() if v == maximum]\n",
    "    \n",
    "    if len(d_max)==1:\n",
    "        return d_max[0]\n",
    "    else:\n",
    "        return np.random.choice(d_max,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Return six most popular products\n",
    "def MostPopularRec(num):\n",
    "    rec=[]\n",
    "    idx=np.random.randint(0,6,num)\n",
    "    for i in idx:\n",
    "        tmp=Productdata[Productdata['subcategory']==Popularity['subcategory'][i]]\n",
    "        tmp_s=sorted(tmp['Popularity score'],reverse=True)\n",
    "        rec.append(tmp.ix[tmp[tmp['Popularity score']==tmp_s[0]].index[0],'tid'])\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return the product purchased by expert of this subcategory\n",
    "def ExpertRec(browse):\n",
    "\n",
    "    expert_list=list(Expert['subcategory'])\n",
    "    rec=[]\n",
    "    exp=[]\n",
    "    candidate=[]\n",
    "    for record in set(browse):\n",
    "        sub_cat=Productdata[Productdata['tid']==record]['subcategory'].values[0]\n",
    "        #Not all categpries have an expert so we have to use a if statement\n",
    "        if sub_cat in expert_list:\n",
    "             exp.extend(Expert[Expert['subcategory']==sub_cat]['UID'].values[0].split(\",\"))\n",
    "    rec_num=0\n",
    "    \n",
    "    for ex in exp:\n",
    "        candidate.extend(Userdata.iloc[int(ex)]['tid'].split(\",\"))\n",
    "    #if there is no expert in subcategory or the number of purchase of expert is less than six, then ask help from MostRec\n",
    "    if len(candidate)<6:\n",
    "        rec.extend(candidate)\n",
    "        rec.extend(MostPopularRec(6-len(candidate)))\n",
    "    else:\n",
    "        shuffle(candidate)\n",
    "        rec.extend(candidate[0:6])\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The similarity calculator for Designer,Product and User\n",
    "def SimCal(uid_target,cand_list,top_num,mode):\n",
    "    #Used for designer and user\n",
    "    if mode!='Product':\n",
    "        Uc=Userdata.ix[uid_target,'subcategory']\n",
    "        Us=Userdata.ix[uid_target,'Product Tag']\n",
    "\n",
    "        Uc_dic=dic_converter(Uc)\n",
    "        max_c=max(Uc_dic.values())\n",
    "        for k in Uc_dic.keys():\n",
    "            Uc_dic[k]=float(Uc_dic[k])/float(max_c)\n",
    "\n",
    "        Us_dic=dic_converter(Us)\n",
    "        \n",
    "        #Calculating of the subcategory similarity\n",
    "        Cat_dic=[dic_converter(e) for e in Productdata.ix[cand_list,'subcategory']]\n",
    "        Cat_sim=[dic_sim(Uc_dic,e) for e in Cat_dic]\n",
    "        #Calculation of the style similarity\n",
    "        Style_dic=[dic_converter(e) for e in Productdata.ix[cand_list,'Product Tag']]\n",
    "        Style_sim=[dic_sim(Us_dic,e) for e in Style_dic]\n",
    "        \n",
    "        score=list(np.array(Cat_sim)+np.array(Style_sim))\n",
    "        score_uniqsort=sorted(list(set(score)),reverse=True)\n",
    "        usr_rec_idx=[]\n",
    "        kk=0\n",
    "        while len(usr_rec_idx)<top_num:\n",
    "            usr_rec_idx.extend([i for i,val in enumerate(score) if val==score_uniqsort[kk]])\n",
    "            kk+=1\n",
    "        return [e for e in cand_list if cand_list.index(e) in usr_rec_idx[0:top_num]]\n",
    "    else:\n",
    "        #This part is for similarity of product. When calculating product, we only have to condiser style similarity since the subcategory is all\n",
    "        #the same\n",
    "        Us=Userdata.ix[uid_target,'Product Tag']\n",
    "        weight_s=Userdata.ix[uid_target,'Style weight']\n",
    "        Us_dic=dic_converter(Us)\n",
    "        \n",
    "        \n",
    "        Style_dic=[dic_converter(e) for e in Productdata.ix[cand_list,'Product Tag']]\n",
    "        Style_sim=[dic_sim(Us_dic,e) for e in Style_dic]\n",
    "\n",
    "        score_uniqsort=sorted(list(set(Style_sim)),reverse=True)\n",
    "        pd_rec_idx=[]\n",
    "        kk=0\n",
    "        while len(pd_rec_idx)<top_num:\n",
    "            pd_rec_idx.extend([i for i,val in enumerate(Style_sim) if val==score_uniqsort[kk]])\n",
    "            kk+=1\n",
    "        return [e for e in cand_list if cand_list.index(e) in pd_rec_idx[0:top_num]]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UserRec(usr,k):\n",
    "    #filter out the product already seen by user\n",
    "    rec=[]\n",
    "    bought=usr['tid'].split(\",\")\n",
    "    if usr['Browse']==u'0':\n",
    "        seen=set(bought)\n",
    "    else:\n",
    "        browse=usr['Browse'].split(\",\")\n",
    "        seen=set(bought).union(set(browse))\n",
    "    \n",
    "    #Extract the product list from 2 most similar user\n",
    "    usr_sim=UserSim.iloc[usr['UID']]\n",
    "    sim_item=[]\n",
    "    for u in usr_sim:\n",
    "        sim_item.extend(Userdata.ix[u,'tid'].split(\",\"))\n",
    "\n",
    "    candidate=[e for e in list(set(sim_item).difference(seen)) if e!=u'0']\n",
    "    \n",
    "    #if the number of candidate is smaller than 6 then ask help from MostPopularRec\n",
    "    if len(candidate)<k:\n",
    "        rec.extend(candidate)\n",
    "        rec.extend(MostPopularRec(k-len(candidate)))\n",
    "    else:\n",
    "        rec.extend(SimCal(usr['UID'],candidate,k,'User'))\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ProductRec(usr,k,rcl,stage):\n",
    "    #filter out the product already seen by user\n",
    "    rec=[]\n",
    "    bought=usr['tid'].split(\",\")\n",
    "    if usr['Browse']==u'0':\n",
    "        seen=set(bought)\n",
    "    else:\n",
    "        browse=usr['Browse'].split(\",\")\n",
    "        seen=set(bought).union(set(browse))\n",
    "    \n",
    "    #Put the products seen by user and the previous rec_list together to form invalid, which means \n",
    "    invalid=set(rcl).union(seen)\n",
    "    Uc=usr['subcategory']\n",
    "    \n",
    "    Uc_dic=dic_converter(Uc)\n",
    "    max_c=dic_max(Uc_dic)\n",
    "    candidate=list(set(Productdata[Productdata['subcategory']==max_c]['tid']).difference(invalid))\n",
    "    #I only consider 5000 products, It is used to reduce product space, trade of between response time and accuracy.\n",
    "    #By doing so, it can also reduce overspecialization since we add randomness\n",
    "    if len(candidate)>5000-stage*1000:\n",
    "        shuffle(candidate)\n",
    "        candidate_rnd=candidate[0:5000-stage*1000]\n",
    "    else:\n",
    "        candidate_rnd=candidate\n",
    "        \n",
    "    rec.extend(SimCal(usr['UID'],candidate_rnd,k,'Product'))\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DesignerRec(usr,k,rcl):\n",
    "    #Filter out the product seen by user\n",
    "    rec=[]\n",
    "    bought=usr['tid'].split(\",\")\n",
    "    if usr['Browse']==u'0':\n",
    "        seen=set(bought)\n",
    "    else:\n",
    "        browse=usr['Browse'].split(\",\")\n",
    "        seen=set(bought).union(set(browse))\n",
    "    \n",
    "    invalid=set(rcl).union(seen)\n",
    "    designer_dic=dic_converter(usr['designer'])\n",
    "    \n",
    "    #print designer_dic\n",
    "    for dk in designer_dic.keys():\n",
    "        if dk not in list(Designer_data[0]):\n",
    "            del designer_dic[dk]\n",
    "    #print designer_dic.keys()\n",
    "    max_d=dic_max(designer_dic)\n",
    "    \n",
    "    #print max_d\n",
    "    d_idx=Designer_data[Designer_data[0]==max_d].index[0]\n",
    "    d_idxsim=Designer_sim.ix[d_idx,3]\n",
    "\n",
    "    cand_desinger=list(Designer_data.ix[[d_idx,d_idxsim],0])\n",
    "    cand=[]\n",
    "    for d in cand_desinger:\n",
    "        cand.extend(list(Productdata[Productdata['owner']==d]['tid']))\n",
    "    candidate=list(set(cand).difference(invalid))\n",
    "\n",
    "    rec.extend(np.random.choice(candidate,k,replace=False))\n",
    "    return rec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Recommender(usr,rand_p):\n",
    "    rec_list=[]\n",
    "    if (usr['tid']==u'0') and (usr['Browse']==u'0'):\n",
    "        print \"Trigger MostPopularRec Engine\"\n",
    "        rec_list=MostPopularRec(6)\n",
    "        \n",
    "    elif (usr['tid']==u'0') and (len(usr['Browse'].split(\",\"))>=1):\n",
    "        print \"Trigger ExpertRec Engine\"\n",
    "        rec_list=ExpertRec(usr['Browse'].split(\",\"))\n",
    "    else:\n",
    "        purchase=len(usr['tid'].split(\",\"))\n",
    "        try:\n",
    "            if purchase<P_stat[0]:\n",
    "                print \"Trigger stage 1\"\n",
    "                rec_list.extend(UserRec(usr,3))\n",
    "                rec_list.extend(np.random.choice(ProductRec(usr,3*(rand_p),rec_list,1),3,replace=False))\n",
    "\n",
    "            elif  P_stat[0]<purchase<P_stat[1]:\n",
    "                print \"Trigger stage 2\"\n",
    "                rec_list.extend(UserRec(usr,2))\n",
    "                rec_list.extend(np.random.choice(ProductRec(usr,3*(rand_p-1),rec_list,2),3,replace=False))\n",
    "                rec_list.extend(DesignerRec(usr,1,rec_list))\n",
    "\n",
    "            elif P_stat[1]<purchase<P_stat[2]:\n",
    "                print \"Trigger stage 3\"\n",
    "                rec_list.extend(UserRec(usr,2))\n",
    "                rec_list.extend(np.random.choice(ProductRec(usr,2*(rand_p-2),rec_list,3),2,replace=False))\n",
    "                rec_list.extend(np.random.choice(DesignerRec(usr,2*(rand_p-2),rec_list),2,replace=False))\n",
    "\n",
    "            else:\n",
    "                print \"Trigger stage 4\"\n",
    "                rec_list.extend(UserRec(usr,1))\n",
    "                rec_list.extend(np.random.choice(ProductRec(usr,2*(rand_p-2),rec_list,4),2,replace=False))\n",
    "                rec_list.extend(DesignerRec(usr,3,rec_list))\n",
    "        except:\n",
    "                print \"Not enough recommend list due to some population is less than sample, call MostPopularRec for help \"\n",
    "                rec_list.extend(MostPopularRec(6-len(rec_list)))\n",
    "    if len(rec_list)<6:\n",
    "        rec_list.extend(MostPopularRec(6-len(rec_list)))\n",
    "\n",
    "    return rec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Randomly choose an user\n",
    "Usr_rnd=Userdata.ix[np.random.randint(0,len(Userdata),1),:]\n",
    "\n",
    "#Print user info\n",
    "print \"tid\"\n",
    "print Userdata.ix[int(Usr_rnd['UID']),'tid']\n",
    "\n",
    "print \"Subcategory\"\n",
    "print Userdata.ix[int(Usr_rnd['UID']),'subcategory']\n",
    "print \"Product Tag\"\n",
    "print Userdata.ix[int(Usr_rnd['UID']),'Product Tag']\n",
    "\n",
    "#Recommend product to user\n",
    "start_time = time.time()\n",
    "Rec_list=Recommender(Userdata.ix[int(Usr_rnd['UID']),:],9)\n",
    "print \"Respond time %s seconds\" % (time.time() - start_time)\n",
    "\n",
    "#Show product link\n",
    "url=['https://www.pinkoi.com/product/'+e for e in Rec_list]\n",
    "for u in url:\n",
    "    print u"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
